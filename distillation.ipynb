{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, beta=0.3, gamma=0.2, temp=3.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.temp = temp\n",
    "\n",
    "    def forward(self, student_outputs, teacher_outputs, labels):\n",
    "        # 硬目标损失\n",
    "        ce_loss = F.cross_entropy(\n",
    "            student_outputs.logits.view(-1, student_outputs.logits.size(-1)),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        # 软目标损失\n",
    "        soft_teacher = F.softmax(teacher_outputs.logits / self.temp, dim=-1)\n",
    "        soft_student = F.log_softmax(student_outputs.logits / self.temp, dim=-1)\n",
    "        kl_loss = F.kl_div(soft_student, soft_teacher, reduction=\"batchmean\") * (self.temp**2)\n",
    "        \n",
    "        # 中间层对齐损失\n",
    "        hidden_loss = F.mse_loss(\n",
    "            student_outputs.hidden_states[-1],  # 取最后一层隐藏状态\n",
    "            self.proj_layer(teacher_outputs.hidden_states[-1])  # 可学习的投影矩阵\n",
    "        )\n",
    "        \n",
    "        return self.alpha*ce_loss + self.beta*kl_loss + self.gamma*hidden_loss\n",
    "    \n",
    "def generate_soft_labels(batch):\n",
    "    inputs = tokenizer(\n",
    "        [f\"{ins} {inp}\" for ins, inp in zip(batch[\"instruction\"], batch[\"input\"])],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = teacher_model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    # 提取logits和最后一层隐藏状态\n",
    "    return {\n",
    "        \"logits\": outputs.logits.cpu(),\n",
    "        \"hidden_states\": outputs.hidden_states[-1].cpu()\n",
    "    }\n",
    "\n",
    "# 对数据集批量处理\n",
    "soft_dataset = dataset.map(\n",
    "    generate_soft_labels,\n",
    "    batched=True,\n",
    "    batch_size=4,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,  # 4bit量化节省显存\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\")\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# 原始LLaMA-2配置\n",
    "original_config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "# 学生模型配置调整\n",
    "student_config = original_config.copy()\n",
    "student_config.update({\n",
    "    \"num_hidden_layers\": 12,  # 从32层减至12层\n",
    "    \"intermediate_size\": 2048,  # FFN维度减半\n",
    "    \"num_attention_heads\": 16  # 注意力头数减半\n",
    "})\n",
    "\n",
    "# 初始化学生模型\n",
    "student_model = AutoModelForCausalLM.from_config(student_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distill_results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=16,  # 有效批次大小=128\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    max_steps=5000,\n",
    "    gradient_checkpointing=True  # 节省显存\n",
    ")\n",
    "\n",
    "# 动态温度调度函数\n",
    "def dynamic_temperature_schedule(step, total_steps):\n",
    "    initial_temp = 5.0\n",
    "    final_temp = 1.0\n",
    "    # 余弦退火调整\n",
    "    temp = final_temp + 0.5 * (initial_temp - final_temp) * (1 + np.cos(np.pi * step / total_steps))\n",
    "    return temp\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # 解包数据\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        teacher_logits = inputs.pop(\"teacher_logits\")\n",
    "        teacher_hidden = inputs.pop(\"teacher_hidden\")\n",
    "\n",
    "        # 学生模型前向\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "        # 构建教师输出对象\n",
    "        teacher_outputs = type(outputs)(\n",
    "            logits=teacher_logits,\n",
    "            hidden_states=[teacher_hidden]\n",
    "        )\n",
    "\n",
    "        # 动态调整温度\n",
    "        current_step = self.state.global_step  # 当前训练步数\n",
    "        max_steps = self.args.max_steps  # 最大训练步数\n",
    "        current_temp = dynamic_temperature_schedule(current_step, max_steps)\n",
    "\n",
    "        # 设置损失函数的温度\n",
    "        self.loss_fn.temp = current_temp\n",
    "\n",
    "        # 计算复合损失\n",
    "        loss = self.loss_fn(outputs, teacher_outputs, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def __init__(self, *args, loss_fn=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fn = loss_fn  # 传递损失函数\n",
    "\n",
    "# 初始化损失函数\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temperature = 1.0  # 默认温度\n",
    "        self.kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "    def forward(self, student_outputs, teacher_outputs, labels=None):\n",
    "        # 计算学生和教师的 logits\n",
    "        student_logits = student_outputs.logits / self.temperature\n",
    "        teacher_logits = teacher_outputs.logits / self.temperature\n",
    "\n",
    "        # 计算 KL 散度损失\n",
    "        kl_loss = self.kl_loss(\n",
    "            F.log_softmax(student_logits, dim=-1),\n",
    "            F.softmax(teacher_logits, dim=-1)\n",
    "        )\n",
    "\n",
    "        # 如果有标签，计算交叉熵损失\n",
    "        if labels is not None:\n",
    "            ce_loss = F.cross_entropy(student_logits, labels)\n",
    "            return kl_loss + ce_loss  # 或者根据需\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
