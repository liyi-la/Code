{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 旋转位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sinusoidal_pos_emedbedding(bs, seq_len, nums_head, dmodel):\n",
    "    position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(-1)## [seq_len, 1]\n",
    "    ids = torch.arange(0, dmodel//2, dtype = torch.float) ##公式中的i\n",
    "    theta = torch.pow(10000, -2 * ids / dmodel) ##公式中的theta,和每个token维度有关\n",
    "    embeddings = position * theta ## [seq_len, dmodel//2]   对应论文公式中的sin(θ * i)\n",
    "    embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1) ## [seq_len, dmodel//2, 2]\n",
    "    embeddings = embeddings.unsqueeze(0).repeat(bs, nums_head, 1, 1, 1) ## [bs, nums_head, seq_len, dmodel//2, 2] 在bs和head维度上进行重复\n",
    "    embeddings = torch.reshape(embeddings, (bs, nums_head, seq_len, dmodel))## [bs, nums_head, seq_len, dmodel],恢复为原始维度，在最后一个维度，偶数sin，奇数cos\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def RoPE(q, k):\n",
    "    bs = q.size(0)\n",
    "    seq_len = q.size(1)\n",
    "    nums_head = q.size(2)\n",
    "    dmodel = q.size(3)\n",
    "    pos_emb = sinusoidal_pos_emedbedding(bs, seq_len, nums_head, dmodel)\n",
    "    \n",
    "    cos_pos = pos_emb[..., 1::2].repeat_interleave(2, dim=-1) ## [bs, nums_head, seq_len, dmodel]奇数列拿来复制\n",
    "    sin_pos = pos_emb[..., ::2].repeat_interleave(2, dim=-1) ## [bs, nums_head, seq_len, dmodel]偶数列\n",
    "    \n",
    "    q2 = torch.stack(-q[..., 1::2], q[..., ::2], dim=-1)#正负交替（对于sin）\n",
    "    q2 = q2.reshape(q.shape)\n",
    "    q = q * cos_pos + q2 * sin_pos # [bs, nums_head, seq_len, dmodel]\n",
    "    \n",
    "    \n",
    "    k2 = torch.stack([-k[..., 1::2], k[..., ::2]], dim=-1)\n",
    "    k2 = k2.reshape(k.shape)\n",
    "    # 更新kw, *对应位置相乘\n",
    "    k = k * cos_pos + k2 * sin_pos # [bs, nums_head, seq_len, dmodel]\n",
    "    \n",
    "    return q, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 5, 5, 2])\n",
      "torch.Size([8, 4, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "position = torch.arange(0, 5, dtype = torch.float).unsqueeze(-1)\n",
    "ids = torch.arange(0, 10//2, dtype = torch.float)\n",
    "theta = torch.pow(10000, -2 * ids / 10)\n",
    "embeddings = position * theta\n",
    "embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "# embeddings = embeddings.repeat((8, 4, *([1] * len(embeddings.shape)))) \n",
    "embeddings = embeddings.unsqueeze(0).repeat(8, 4, 1, 1,1)\n",
    "print(embeddings.shape)\n",
    "embeddings = torch.reshape(embeddings, (8, 4, 5, 10))\n",
    "print(embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
